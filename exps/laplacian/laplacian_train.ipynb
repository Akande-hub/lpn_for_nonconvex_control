{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mtd5gx1tv90z"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from network import LPN\n",
    "from utils import prox, cvx, prior, gt_cvx, soft_thr\n",
    "\n",
    "sns.set()\n",
    "\n",
    "MODEL_DIR = \"experiments/models/\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# set seed\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data points from Laplacian distribution:\n",
    "# exp(-|x|/b) / (2b)\n",
    "def sample_laplace(n, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        n: number of points\n",
    "        b: scale parameter of the Laplacian distribution\n",
    "    Outputs:\n",
    "        x: a vector of n points, torch.tensor\n",
    "    \"\"\"\n",
    "    x = torch.distributions.laplace.Laplace(0, b).sample((n,))\n",
    "    return x\n",
    "\n",
    "\n",
    "class LaplaceSampler(object):\n",
    "    def __init__(self, b):\n",
    "        self.b = b\n",
    "\n",
    "    def __call__(self, n):\n",
    "        return sample_laplace(n, self.b)\n",
    "\n",
    "\n",
    "b = 1.0\n",
    "dist = torch.distributions.laplace.Laplace(0, b)\n",
    "x = LaplaceSampler(b)(10000)\n",
    "# plot pdf, log-likelihood, cdf\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 2)\n",
    "x = torch.linspace(-1, 1, 100)\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(x, dist.log_prob(x).exp())\n",
    "plt.title(\"PDF\")\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(x, dist.log_prob(x))\n",
    "plt.title(\"Log-likelihood\")\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(x, dist.cdf(x))\n",
    "plt.title(\"CDF\")\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.hist(sample_laplace(10000, b), bins=100)\n",
    "plt.title(\"Histogram\")\n",
    "plt.suptitle(\"Laplace distribution\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the data\n",
    "def add_noise(x, sigma=0.1):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x: a vector of n points, torch.tensor\n",
    "        sigma: standard deviation of the noise\n",
    "    Outputs:\n",
    "        x: a vector of n points, torch.tensor\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(x) * sigma\n",
    "    return x + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for laplace distribution\n",
    "b = 1\n",
    "sigma_noise = 1\n",
    "sampler = LaplaceSampler(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example training points\n",
    "target = sampler(200).unsqueeze(1)\n",
    "input = add_noise(target, sigma_noise)\n",
    "plt.rcParams[\"figure.figsize\"] = (3, 3)\n",
    "plt.scatter(input, target, s=10, facecolors=\"none\", edgecolors=\"tab:blue\")\n",
    "plt.grid(\"on\")\n",
    "plt.title(\"Example training points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot learned prox, convex function, original function\n",
    "def plot_all(model):\n",
    "    xi = np.linspace(-4, 4, 1000)\n",
    "\n",
    "    y = prox(xi, model)\n",
    "    c = cvx(xi, model)\n",
    "    p = prior(xi, model)\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 3)\n",
    "\n",
    "    # learned prox\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(xi, y, \"-\", ms=5, label=\"LPN\")\n",
    "    plt.plot(xi, soft_thr(xi), \"--\", label=\"soft thr.\", zorder=1.9)\n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Learned prox\")\n",
    "\n",
    "    # convex function\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(xi, c - c.min(), \"-\", ms=5, label=\"Cvx func\")\n",
    "    plt.plot(xi, gt_cvx(xi), \"--\", label=\"ref.\", zorder=1.9)\n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Cvx func\")\n",
    "\n",
    "    # original function\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(xi, p - p.min(), label=\"LPN\")\n",
    "    plt.plot(xi, np.abs(xi), \"--\", label=r\"$\\ell_1$\", zorder=1.9)\n",
    "    plt.grid(\"on\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Original func\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proximal matching loss: -exp(-(x/gamma)^2) + 1\n",
    "def exp_func(x, gamma):\n",
    "    return -torch.exp(-((x / gamma) ** 2)) + 1\n",
    "\n",
    "\n",
    "# visualize the loss at different sigma\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 2)\n",
    "x = torch.linspace(-8, 8, 101)\n",
    "for gamma_exp_loss in [0.2, 0.5, 1, 2]:\n",
    "    plt.plot(x, exp_func(x, gamma_exp_loss), label=\"gamma=\" + str(gamma_exp_loss))\n",
    "plt.legend()\n",
    "plt.grid(\"on\")\n",
    "plt.title(\"exp loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiVg2pVgH2jM"
   },
   "outputs": [],
   "source": [
    "beta = 10  # beta of softplus\n",
    "hidden = 50  # number of hidden units\n",
    "layers = 4  # number of layers\n",
    "\n",
    "# create a model\n",
    "lpn_model = LPN(in_dim=dim, hidden=hidden, layers=layers, beta=beta).to(device)\n",
    "print(\n",
    "    \"Number of parameters in ICNN\",\n",
    "    sum(p.size().numel() for p in lpn_model.parameters()),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_iteration(i, bsize, sigma_noise, optimizer, loss_type, gamma_loss=None):\n",
    "    target = sampler(bsize).unsqueeze(1)\n",
    "    input = add_noise(target, sigma_noise)\n",
    "    input, target = input.to(device), target.to(device)\n",
    "    out = lpn_model(input)\n",
    "\n",
    "    if loss_type == 2:\n",
    "        loss = (out - target).pow(2).sum() / bsize  # MSE loss\n",
    "    elif loss_type == 1:\n",
    "        loss = (out - target).abs().sum() / bsize  # MAE loss\n",
    "    elif loss_type == 0:\n",
    "        loss = exp_func(out - target, gamma_loss).mean()  # proximal matching loss\n",
    "    else:\n",
    "        raise ValueError(\"loss_type must be 0, 1, or 2\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    lpn_model.wclip()\n",
    "\n",
    "    # monitor\n",
    "    # loss_lstar = exp_func(out - target, 0.1).mean()\n",
    "\n",
    "    # if not i % 500:\n",
    "    #     print(\"iteration\", i, \"loss\", loss.item(), \"loss_lstar\", loss_lstar.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "lpn_model = LPN(in_dim=dim, hidden=hidden, layers=layers, beta=beta).to(device)\n",
    "optimizer = torch.optim.Adam(lpn_model.parameters(), lr=1e-3)\n",
    "bsize = 2000\n",
    "\n",
    "for i in range(10000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type=2)\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-4\n",
    "for i in range(10000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type=2)\n",
    "\n",
    "\n",
    "# save the model\n",
    "torch.save(lpn_model.state_dict(), os.path.join(MODEL_DIR, \"l2.pth\"))\n",
    "\n",
    "plot_all(lpn_model)\n",
    "print(\n",
    "    \"Values at [-3,-2,-1,0,1,2,3]:\",\n",
    "    lpn_model(\n",
    "        torch.tensor([-3, -2, -1, 0, 1, 2, 3]).unsqueeze(1).float().to(device)\n",
    "    ).squeeze(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "lpn_model = LPN(in_dim=dim, hidden=hidden, layers=layers, beta=beta).to(device)\n",
    "optimizer = torch.optim.Adam(lpn_model.parameters(), lr=1e-3)\n",
    "bsize = 2000\n",
    "\n",
    "for i in range(10000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type=1)\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-4\n",
    "for i in range(10000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type=1)\n",
    "\n",
    "\n",
    "# save the model\n",
    "torch.save(lpn_model.state_dict(), os.path.join(MODEL_DIR, \"l1.pth\"))\n",
    "\n",
    "plot_all(lpn_model)\n",
    "print(\n",
    "    \"Values at [-3,-2,-1,0,1,2,3]:\",\n",
    "    lpn_model(\n",
    "        torch.tensor([-3, -2, -1, 0, 1, 2, 3]).unsqueeze(1).float().to(device)\n",
    "    ).squeeze(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal matching loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_type = 0\n",
    "gamma_loss = 0.5\n",
    "\n",
    "# set learning rate\n",
    "for g in optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-3\n",
    "\n",
    "for i in range(2000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type, gamma_loss)\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-4\n",
    "\n",
    "for i in range(2000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type, gamma_loss)\n",
    "\n",
    "torch.save(lpn_model.state_dict(), os.path.join(MODEL_DIR, f\"l_{gamma_loss}.pth\"))\n",
    "\n",
    "plot_all(lpn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_loss = 0.4\n",
    "for g in optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-4\n",
    "\n",
    "for i in range(4000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type, gamma_loss)\n",
    "\n",
    "torch.save(lpn_model.state_dict(), os.path.join(MODEL_DIR, f\"l_{gamma_loss}.pth\"))\n",
    "\n",
    "plot_all(lpn_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_loss = 0.3\n",
    "for g in optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-4\n",
    "\n",
    "for i in range(4000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type, gamma_loss)\n",
    "\n",
    "torch.save(lpn_model.state_dict(), os.path.join(MODEL_DIR, f\"l_{gamma_loss}.pth\"))\n",
    "\n",
    "plot_all(lpn_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_loss = 0.2\n",
    "for g in optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-5\n",
    "\n",
    "for i in range(4000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type, gamma_loss)\n",
    "\n",
    "torch.save(lpn_model.state_dict(), os.path.join(MODEL_DIR, f\"l_{gamma_loss}.pth\"))\n",
    "\n",
    "plot_all(lpn_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gamma=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_loss = 0.1\n",
    "for g in optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-5\n",
    "\n",
    "for i in range(4000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type, gamma_loss)\n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g[\"lr\"] = 1e-6\n",
    "\n",
    "for i in range(4000):\n",
    "    single_iteration(i, bsize, sigma_noise, optimizer, loss_type, gamma_loss)\n",
    "\n",
    "torch.save(lpn_model.state_dict(), os.path.join(MODEL_DIR, f\"l_{gamma_loss}.pth\"))\n",
    "\n",
    "plot_all(lpn_model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.16 ('lpn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf6728946baef40d9b92029dfd442e263b36891aa3f1a4a738fab944669af8ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
